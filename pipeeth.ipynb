{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880617a-eb7a-43e5-ade1-305eb52ecbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_set, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, LongType\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Start time\")\n",
    "# Constants\n",
    "INFURA_URLS = [\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "def get_next_infura_url():\n",
    "    global infura_url_index\n",
    "    url = INFURA_URLS[infura_url_index]\n",
    "    infura_url_index = (infura_url_index + 1) % len(INFURA_URLS)\n",
    "    \n",
    "    return url\n",
    "\n",
    "def get_latest_block_number():\n",
    "    infura_url = \"\"\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_blockNumber\",\n",
    "        \"params\": [],\n",
    "        \"id\": 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(infura_url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
    "        response_json = response.json()\n",
    "        if 'error' not in response_json:\n",
    "            return int(response_json['result'], 16)  # Convert hex to int\n",
    "        else:\n",
    "            logging.error(\"Error fetching latest block number: {}\".format(response_json['error']))\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(\"Error making request to fetch latest block number: {}\".format(e))\n",
    "        return None\n",
    "\n",
    "# BLOCK RANGE\n",
    "LATEST_BLOCK_NUMBER = get_latest_block_number()\n",
    "print(LATEST_BLOCK_NUMBER)\n",
    "\n",
    "STARTING_BLOCK_NUMBER = LATEST_BLOCK_NUMBER -3000 #300b/h\n",
    "\n",
    "TRANSFER_EVENT_SIGNATURE = \"\"\n",
    "infura_url_index = 0\n",
    "\n",
    "# Schema definition\n",
    "\n",
    "erc20_transaction_schema = StructType([\n",
    "    StructField(\"blockHash\", StringType(), True),\n",
    "    StructField(\"blockNumber\", StringType(), True),\n",
    "    StructField(\"from\", StringType(), True),\n",
    "    StructField(\"gas\", StringType(), True),\n",
    "    StructField(\"gasPrice\", StringType(), True),\n",
    "    StructField(\"transactionHash\", StringType(), True),\n",
    "    StructField(\"input\", StringType(), True),\n",
    "    StructField(\"nonce\", StringType(), True),\n",
    "    StructField(\"to\", StringType(), True),\n",
    "    StructField(\"transactionIndex\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"v\", StringType(), True),\n",
    "    StructField(\"r\", StringType(), True),\n",
    "    StructField(\"s\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"data\", StringType(), True),\n",
    "    StructField(\"topics\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_string_integers(data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, int):\n",
    "            data[key] = str(value)\n",
    "\n",
    "        elif isinstance(value, dict):\n",
    "            data[key] = ensure_string_integers(value)\n",
    "\n",
    "        elif isinstance(value, list):\n",
    "            for idx, item in enumerate(value):\n",
    "                if isinstance(item, int):\n",
    "                    value[idx] = str(item)\n",
    "                elif isinstance(item, dict):\n",
    "                    value[idx] = ensure_string_integers(item)\n",
    "    return data\n",
    "\n",
    "\n",
    "def left_pad_address(address):\n",
    "    return \"0x\" + address[2:].rjust(64, '0')\n",
    "\n",
    "\n",
    "def is_potential_dusting_attack(transaction):\n",
    "    UNIQUE_RECIPIENT_THRESHOLD = 2\n",
    "    unique_recipients = set()\n",
    "    for topic in transaction.get(\"topics\", []):\n",
    "        if topic[0] == TRANSFER_EVENT_SIGNATURE:\n",
    "            recipient = topic[2]\n",
    "            unique_recipients.add(recipient)\n",
    "            \n",
    "    return len(unique_recipients) > UNIQUE_RECIPIENT_THRESHOLD\n",
    "\n",
    " # Helper function to extract additional fields\n",
    "def add_additional_fields(transactions):\n",
    "    for txn in transactions:\n",
    "        txn[\"address\"] = txn.get(\"address\", None)\n",
    "        txn[\"topics\"] = txn.get(\"topics\", [])\n",
    "        txn[\"data\"] = txn.get(\"data\", None)\n",
    "        txn[\"transactionHash\"] = txn.get(\"transactionHash\", None)\n",
    "    return transactions\n",
    "\n",
    "def fetch_transactions_for_address(address):\n",
    "    transactions = []\n",
    "    infura_url = get_next_infura_url()\n",
    "    address = left_pad_address(address)\n",
    "    #must be splited if an address has too many txs\n",
    "    block_ranges = [(STARTING_BLOCK_NUMBER, LATEST_BLOCK_NUMBER)]\n",
    "\n",
    "    for start_block, end_block in block_ranges:\n",
    "        # First request: address as the sender\n",
    "        data_from = {\n",
    "            \"jsonrpc\": \"2.0\",            \n",
    "            \"id\": 1,\n",
    "            \"method\": \"eth_getLogs\",\n",
    "            \"params\": [{\n",
    "                \"fromBlock\": hex(start_block),\n",
    "                \"toBlock\": hex(end_block),\n",
    "                \"topics\": [TRANSFER_EVENT_SIGNATURE, address, None]\n",
    "            }]\n",
    "        }\n",
    "        data_from = ensure_string_integers(data_from)\n",
    "\n",
    "        # Second request: address as the recipient\n",
    "        data_to = {\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": 1,\n",
    "            \"method\": \"eth_getLogs\",\n",
    "            \"params\": [{\n",
    "                \"fromBlock\": hex(start_block),\n",
    "                \"toBlock\": hex(end_block),\n",
    "                \"topics\": [TRANSFER_EVENT_SIGNATURE, None, address]\n",
    "            }]\n",
    "        }\n",
    "        data_to = ensure_string_integers(data_to)\n",
    "\n",
    "       \n",
    "\n",
    "        # Fetch transactions where address is the sender\n",
    "        try:\n",
    "            response = requests.post(infura_url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data_from))\n",
    "            response_json = response.json()\n",
    "            #logging.info(f\"Fetched {len(response_json.get('result', []))} transactions FROM address {address}\")\n",
    "            \n",
    "            if 'error' in response_json:\n",
    "                logging.error(f\"API Error for address {address}: {response_json['error']}\")\n",
    "            else:\n",
    "                transactions.extend(add_additional_fields(response_json.get('result', [])))\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            logging.warning(f\"Error fetching transactions FROM address {address}. Error: {e}\")\n",
    "\n",
    "        # Fetch transactions where address is the recipient\n",
    "        try:\n",
    "            response = requests.post(infura_url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data_to))\n",
    "            response_json = response.json()\n",
    "\n",
    "            #logging.info(f\"Fetched {len(response_json.get('result', []))} transactions TO address {address}\")\n",
    "\n",
    "            if 'error' in response_json:\n",
    "                logging.error(f\"API Error for address {address}: {response_json['error']}\")\n",
    "            else:\n",
    "                recipient_transactions = add_additional_fields(response_json.get('result', []))\n",
    "                filtered_transactions = [txn for txn in recipient_transactions if not is_potential_dusting_attack(txn)]\n",
    "                #logging.info(f\"Filtered {len(filtered_transactions)} transactions after dusting attack check\")\n",
    "                transactions.extend(filtered_transactions)\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            logging.warning(f\"Error fetching transactions TO address {address}. Error: {e}\")\n",
    "\n",
    "    #logging.info(f\"Total transactions collected for address {address}: {len(transactions)}\")\n",
    "    return transactions\n",
    "\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Ethereum Token Interaction Aggregator\").getOrCreate()\n",
    "\n",
    "# Load CSV and process top holders\n",
    "df_addresses = spark.read.csv(\"export-tokenholders.csv\", header=True, inferSchema=True)\n",
    "top_100_holders = [row[\"HolderAddress\"].lower() for row in df_addresses.limit(250).collect()]\n",
    "\n",
    "# Filter out blacklisted addresses from the top holders\n",
    "#BT\n",
    "blacklisted_addresses = [\"\"]\n",
    "#J\n",
    "#blacklisted_addresses = [\"\"]\n",
    "top_100_holders = [address for address in top_100_holders if address not in blacklisted_addresses]\n",
    "\n",
    "# Flatten the list of transactions and process in Spark\n",
    "\n",
    "all_transactions = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for transactions in executor.map(fetch_transactions_for_address, top_100_holders):\n",
    "        all_transactions.extend(transactions)\n",
    "print(f\"Total number of transactions fetched: {len(all_transactions)}\")\n",
    "\n",
    "df_transactions = spark.createDataFrame(all_transactions, schema=erc20_transaction_schema)\n",
    "\n",
    "# Extract sender, recipient, and amount from topics\n",
    "\n",
    "def extract_sender(topics):\n",
    "    return topics[1][-40:] if len(topics) > 1 else None\n",
    "\n",
    "def extract_recipient(topics):\n",
    "    return topics[2][-40:] if len(topics) > 2 else None\n",
    "\n",
    "def extract_amount(data):\n",
    "    try:\n",
    "        if isinstance(data, str):\n",
    "            # Convert the data to an unsigned integer\n",
    "            amount = int(data, 16)\n",
    "            return amount\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"Error in extracting amount. Data: {data}, Error: {ve}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "extract_amount_udf = udf(extract_amount, LongType())\n",
    "\n",
    "extract_sender_udf = udf(extract_sender, StringType())\n",
    "\n",
    "extract_recipient_udf = udf(extract_recipient, StringType())\n",
    "\n",
    "\n",
    "# Add columns for sender, recipient, and amount\n",
    "df_transactions = df_transactions.withColumn(\"sender\", extract_sender_udf(col(\"topics\")))\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"recipient\", extract_recipient_udf(col(\"topics\")))\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"amount\", extract_amount_udf(col(\"data\")))\n",
    "\n",
    "# Remove duplicate rows based on the transactionHash column\n",
    "df_transactions = df_transactions.dropDuplicates(['transactionHash'])\n",
    "\n",
    "\n",
    "# Extract token addresses from 'address' field and group by them to get counts\n",
    "\n",
    "df_tokens = df_transactions.withColumn(\"token_address\", col(\"address\"))\n",
    "\n",
    "df_token_counts = df_tokens.groupby(\"token_address\").count().sort(col(\"count\").desc())\n",
    "\n",
    "# Aggregate and join on address to get address_count\n",
    "\n",
    "address_aggregation = df_transactions.groupBy(\"address\").agg(collect_set(\"sender\").alias(\"unique_senders\"), collect_set(\"recipient\").alias(\"unique_recipients\"))\n",
    "\n",
    "# merge unique senders and recipients and count them\n",
    "\n",
    "def merge_and_count(unique_senders, unique_recipients):\n",
    "    return len(set(unique_senders) | set(unique_recipients))\n",
    "\n",
    "merge_and_count_udf = udf(merge_and_count, IntegerType())\n",
    "\n",
    "address_aggregation = address_aggregation.withColumn(\"address_count\", merge_and_count_udf(col(\"unique_senders\"), col(\"unique_recipients\")))\n",
    "\n",
    "final_df = df_token_counts.join(address_aggregation, df_token_counts.token_address == address_aggregation.address).select(\"token_address\", \"count\", \"address_count\")\n",
    "\n",
    "final_df = final_df.orderBy(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e6258-c0b4-418d-bd59-f45ebaabc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the call data for ERC20 function signature for `name()` \n",
    "def fetch_token_name(token_address):\n",
    "    call_data = \"0x06fdde03\"\n",
    "    \n",
    "    infura_url = get_next_infura_url()\n",
    "    \n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"eth_call\",\n",
    "        \"params\": [{\n",
    "            \"to\": token_address,\n",
    "            \"data\": call_data\n",
    "        }, \"latest\"]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(infura_url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
    "        response_json = response.json()\n",
    "\n",
    "        # Convert the hex response to ASCII\n",
    "        result = bytes.fromhex(response_json[\"result\"][2:]).decode('utf-8').rstrip('\\0')\n",
    "        return result\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.warning(f\"Error fetching name for token {token_address}. Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def fetch_name(address):\n",
    "        return (address, fetch_token_name(address))\n",
    "    \n",
    "    \n",
    "def fetch_names_concurrently(token_addresses):\n",
    "    token_names = {}\n",
    "    \n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(fetch_name, address): address for address in token_addresses}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            address = futures[future]\n",
    "            try:\n",
    "                token_names[address] = future.result()[1]\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error fetching name for token {address}. Error: {e}\")\n",
    "                token_names[address] = None\n",
    "\n",
    "    return token_names\n",
    "\n",
    "unique_token_addresses = final_df.select(\"token_address\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "token_names_dict = fetch_names_concurrently(unique_token_addresses)\n",
    "\n",
    "# UDF to map token addresses to their names\n",
    "def map_address_to_name(token_address):\n",
    "    return token_names_dict.get(token_address, None)\n",
    "\n",
    "map_address_to_name_udf = udf(map_address_to_name, StringType())\n",
    "final_df = final_df.withColumn(\"token_name\", map_address_to_name_udf(col(\"token_address\")))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "#final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70c2f5-e0c7-4c42-b37e-7973d30c7fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_100_holders = [address[2:] if address.startswith(\"0x\") else address for address in top_100_holders]\n",
    "\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df_transactions.filter(\n",
    "    df_transactions.sender.isin(top_100_holders) | \n",
    "    df_transactions.recipient.isin(top_100_holders)\n",
    ")\n",
    "\n",
    "# Count distinct occurrences for sender and recipient for each contract\n",
    "result = (filtered_df.groupBy(\"address\")\n",
    "          .agg(F.countDistinct(\"sender\").alias(\"unique_sender_count\"), \n",
    "               F.countDistinct(\"recipient\").alias(\"unique_recipient_count\"))\n",
    "         )\n",
    "\n",
    "# Sort by unique_recipient_count in descending order\n",
    "sorted_result = result.sort(F.desc(\"unique_recipient_count\"))\n",
    "\n",
    "# Add token name\n",
    "sorted_result_with_names = sorted_result.withColumn(\"token_name\", map_address_to_name_udf(col(\"address\")))\n",
    "\n",
    "# Display \n",
    "#sorted_result_with_names.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c828e26-a583-4de2-8bb2-a3e2e012f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABI-encoded function signature for `totalSupply()= \"0x18160ddd\"`\n",
    "\n",
    "def fetch_total_supply(token_address):\n",
    "    total_supply_signature = \"0x18160ddd\"\n",
    "    \n",
    "    data_payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"eth_call\",\n",
    "        \"params\": [{\n",
    "            \"to\": token_address,\n",
    "            \"data\": total_supply_signature\n",
    "        }, \"latest\"]\n",
    "    }\n",
    "    \n",
    "    infura_url = get_next_infura_url()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(infura_url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data_payload))\n",
    "        response_json = response.json()\n",
    "        \n",
    "        # Decode to get the total supply\n",
    "        total_supply = int(response_json.get('result', '0x0'), 16)\n",
    "        return token_address, total_supply\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logging.warning(f\"Error fetching total supply for token {token_address}. Error: {e}\")\n",
    "        return token_address, None\n",
    "\n",
    "\n",
    "def fetch_all_total_supplies(token_addresses):\n",
    "    total_supplies = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(fetch_total_supply, address): address for address in token_addresses}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            address, total_supply = future.result()\n",
    "            total_supplies[address] = total_supply\n",
    "\n",
    "    return total_supplies\n",
    "\n",
    "# Fetching total supplies for all unique token addresses\n",
    "unique_token_addresses = final_df.select(\"token_address\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "total_supplies_dict = fetch_all_total_supplies(unique_token_addresses)\n",
    "\n",
    "\n",
    "def map_address_to_supply(token_address):\n",
    "    return total_supplies_dict.get(token_address, None)\n",
    "\n",
    "# Fetching total supplies for all unique token addresses\n",
    "unique_token_addresses = sorted_result_with_names.select(\"address\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "total_supplies_dict = fetch_all_total_supplies(unique_token_addresses)\n",
    "\n",
    "#map token addresses to their supplies\n",
    "def map_address_to_supply(token_address):\n",
    "    return total_supplies_dict.get(token_address, None)\n",
    "\n",
    "map_address_to_supply_udf = udf(map_address_to_supply, StringType())\n",
    "\n",
    "# Add the total_supply column to the sorted_result_with_names DataFrame\n",
    "sorted_result_with_names = sorted_result_with_names.withColumn(\"total_supply\", map_address_to_supply_udf(col(\"address\")))\n",
    "\n",
    "#Show the updated DataFrame with total_supply included\n",
    "#sorted_result_with_names.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a30c6-d5b9-4cc2-96fe-cd8bed9372fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by token_address and aggregate sum of amount for incoming and outgoing transactions\n",
    "agg_sum_df = df_transactions.groupBy(\"address\").agg(\n",
    "    F.sum(F.when(col(\"sender\").isin(top_100_holders), col(\"amount\")).otherwise(0)).alias(\"outgoing_sum\"),\n",
    "    F.sum(F.when(col(\"recipient\").isin(top_100_holders), col(\"amount\")).otherwise(0)).alias(\"incoming_sum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12059f-795d-4414-b26b-4f9329a7b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sorted_result_with_names.join(agg_sum_df, sorted_result_with_names.address == agg_sum_df.address, how=\"left\")\n",
    "\n",
    "# Drop the duplicate address column if it exists after the join\n",
    "final_df = final_df.drop(agg_sum_df.address)\n",
    "\n",
    "# Replace nulls with zeros in the sums \n",
    "final_df = final_df.na.fill({'outgoing_sum': 0, 'incoming_sum': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6e317-9146-44c9-98fc-923b503a96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.withColumn(\"percent_outgoing\", (col(\"outgoing_sum\") / col(\"total_supply\")) * 100)\n",
    "final_df = final_df.withColumn(\"percent_incoming\", (col(\"incoming_sum\") / col(\"total_supply\")) * 100)\n",
    "\n",
    "#filter out contracts with only one sender\n",
    "final_df = final_df.filter(col(\"unique_sender_count\") > 1)\n",
    "\n",
    "#updated DataFrame\n",
    "#final_df.show()\n",
    "\n",
    "final_df = final_df.sort(F.desc(\"unique_recipient_count\"))\n",
    "# Show the updated DataFrame with percentage columns\n",
    "final_df = final_df[['unique_sender_count', 'unique_recipient_count', 'address', 'token_name', 'outgoing_sum', 'incoming_sum', 'percent_outgoing', 'percent_incoming']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd0cb2-b325-4303-8386-fc9de08414c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_final_df = final_df.toPandas()\n",
    "display(pd_final_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfe1b6-188b-4598-9e8c-2d25572873bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the first 50 rows of the DataFrame without truncating\n",
    "final_df = final_df[['unique_sender_count', 'unique_recipient_count', 'token_name', 'address']]\n",
    "final_df.show(50, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
